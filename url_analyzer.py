import joblib
import pandas as pd
import requests
import ssl
import socket
from bs4 import BeautifulSoup
from urllib.parse import urlparse
import re
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from datetime import datetime
import whois
import dns.resolver

# ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø¯Ø±Ø¨
model = joblib.load('voting_classifier_hard.pkl')

phishing_keywords = [
    "password", "verify", "account", "login", "update", "security", "bank", "confirm",
    "click here", "urgent", "limited time", "risk", "suspend", "alert", "failure"
]

nltk.download('punkt')
nltk.download('stopwords')


def setup_selenium():
    options = Options()
    options.add_argument("--headless")
    options.add_argument('--disable-gpu')
    options.add_argument('--no-sandbox')
    driver = webdriver.Chrome(options=options)
    return driver


def check_ssl_cert(url):
    try:
        hostname = urlparse(url).hostname
        context = ssl.create_default_context()
        with socket.create_connection((hostname, 443), timeout=5) as sock:
            with context.wrap_socket(sock, server_hostname=hostname) as ssock:
                cert = ssock.getpeercert()
                not_after = cert.get('notAfter')
                expire_date = datetime.strptime(not_after, '%b %d %H:%M:%S %Y %Z')
                if expire_date < datetime.utcnow():
                    return False, "Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© Ù…Ù†ØªÙ‡ÙŠØ© Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©"
                return True, "Ø§Ù„Ø´Ù‡Ø§Ø¯Ø© ØµØ§Ù„Ø­Ø©"
    except Exception as e:
        return False, f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ø´Ù‡Ø§Ø¯Ø©: {e}"


def analyze_text_content(text):
    text = text.lower()
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    filtered_words = [w for w in tokens if w.isalpha() and w not in stop_words]
    for word in filtered_words:
        if word in phishing_keywords:
            return False, f"ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø© Ù…Ø´Ø¨ÙˆÙ‡Ø©: {word}"
    return True, "Ù„Ø§ ØªÙˆØ¬Ø¯ ÙƒÙ„Ù…Ø§Øª Ù…Ø´Ø¨ÙˆÙ‡Ø©"


def analyze_html(html_content, url):
    soup = BeautifulSoup(html_content, 'html.parser')
    forms = soup.find_all('form')
    for form in forms:
        form_text = form.get_text().lower()
        if any(keyword in form_text for keyword in ["password", "credit card", "ssn", "social security", "pin"]):
            return False, "Ù†Ù…ÙˆØ°Ø¬ ÙŠØ·Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø³Ø§Ø³Ø©"
    scripts = soup.find_all('script')
    for script in scripts:
        if script.string and ('eval(' in script.string or 'unescape(' in script.string):
            return False, "ÙˆØ¬ÙˆØ¯ Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª ØªØ´ÙÙŠØ± Ø£Ùˆ Ø¥Ø®ÙØ§Ø¡ Ù…Ø­ØªÙˆÙ‰"
    anchors = soup.find_all('a', href=True)
    suspicious_domains = 0
    main_domain = urlparse(url).netloc
    for a in anchors:
        link_domain = urlparse(a['href']).netloc
        if link_domain and link_domain != main_domain:
            suspicious_domains += 1
    if suspicious_domains > 15:
        return False, "ÙˆØ¬ÙˆØ¯ Ø±ÙˆØ§Ø¨Ø· ÙƒØ«ÙŠØ±Ø© Ø¥Ù„Ù‰ Ù†Ø·Ø§Ù‚Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ù…Ø´Ø¨ÙˆÙ‡Ø©"
    meta_redirect = soup.find('meta', attrs={'http-equiv': 'refresh'})
    if meta_redirect:
        return False, "ÙˆØ¬ÙˆØ¯ Ø¥Ø¹Ø§Ø¯Ø© ØªÙˆØ¬ÙŠÙ‡ Ø¯Ø§Ø®Ù„ Ø§Ù„ØµÙØ­Ø©"
    return True, "ØªØ­Ù„ÙŠÙ„ HTML Ø·Ø¨ÙŠØ¹ÙŠ"


def get_domain_age(domain):
    try:
        w = whois.whois(domain)
        creation_date = w.creation_date
        if isinstance(creation_date, list):
            creation_date = creation_date[0]
        if creation_date is None:
            return -1  # ØºÙŠØ± Ù…Ø¹Ø±ÙˆÙ
        age_days = (datetime.utcnow() - creation_date).days
        if age_days < 180:  # Ø£Ù‚Ù„ Ù…Ù† 6 Ø£Ø´Ù‡Ø±
            return -1
        elif age_days < 365 * 2:
            return 0
        else:
            return 1
    except Exception:
        return -1


def has_dns_record(domain):
    try:
        answers = dns.resolver.resolve(domain, 'A')
        if answers:
            return 1
        return -1
    except Exception:
        return -1


def extract_features_from_url(url):
    features = {}
    parsed_url = urlparse(url)
    domain = parsed_url.netloc

    ip_pattern = re.compile(r'(\d{1,3}\.){3}\d{1,3}')
    features['having_IP'] = -1 if ip_pattern.search(url) else 1

    length = len(url)
    if length < 54:
        features['URL_Length'] = -1
    elif length <= 75:
        features['URL_Length'] = 0
    else:
        features['URL_Length'] = 1

    short_services = ['bit.ly', 'tinyurl', 'goo.gl', 'ow.ly', 'is.gd', 'buff.ly']
    features['Shortining_Service'] = -1 if any(s in url for s in short_services) else 1

    features['having_At_Symbol'] = -1 if '@' in url else 1

    double_slash_pos = url.find('//', url.find('://') + 3)
    features['double_slash_redirecting'] = 1 if double_slash_pos != -1 else -1

    features['Prefix_Suffix'] = -1 if '-' in domain else 1

    dots = domain.count('.')
    if dots == 1:
        features['having_Sub_Domain'] = 1
    elif dots == 2:
        features['having_Sub_Domain'] = 0
    else:
        features['having_Sub_Domain'] = -1

    features['SSLfinal_State'] = 1 if parsed_url.scheme == 'https' else -1

    # Ø·ÙˆÙ„ Ø§Ù„Ø¯ÙˆÙ…ÙŠÙ†
    features['Domain_registeration_length'] = 1 if len(domain) > 15 else -1

    features['Favicon'] = 1  # ØªØ¨Ø³ÙŠØ·

    port = parsed_url.port
    if port is None or port in [80, 443]:
        features['port'] = 1
    else:
        features['port'] = -1

    features['HTTPS_token'] = -1 if 'https' in domain else 1

    features['Request_URL'] = 1
    features['URL_of_Anchor'] = 1
    features['Links_in_tags'] = 1
    features['SFH'] = 1
    features['Submitting_to_email'] = 1
    features['Abnormal_URL'] = 1
    features['Redirect'] = 0
    features['on_mouseover'] = 0
    features['RightClick'] = 0
    features['popUpWidnow'] = 0
    features['Iframe'] = 0

    # Ø§Ù„Ø¹Ù…Ø± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ WHOIS
    features['age_of_domain'] = get_domain_age(domain)

    # Ø³Ø¬Ù„Ø§Øª DNS Ø­Ù‚ÙŠÙ‚ÙŠØ©
    features['DNSRecord'] = has_dns_record(domain)

    # Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ØªÙŠ Ù„Ø§ ØªØªÙˆÙØ± Ø¨ÙŠØ§Ù†Ø§Øª Ø­Ù‚ÙŠÙ‚ÙŠØ© Ù„Ù‡Ø§: Ù†ÙØªØ±Ø¶ 0 (ÙŠÙ…ÙƒÙ† ØªØ­Ø³ÙŠÙ†Ù‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹)
    features['web_traffic'] = 0
    features['Page_Rank'] = 0
    features['Google_Index'] = 0
    features['Links_pointing_to_page'] = 0
    features['Statistical_report'] = 0

    return pd.DataFrame([features])

import requests
import time

def virustotal_scan(url, api_key):
    headers = {"x-apikey": api_key}
    scan_url = "https://www.virustotal.com/api/v3/urls"

    try:
        # 1. Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ù„Ù„ÙØ­Øµ
        response = requests.post(scan_url, headers=headers, data={'url': url})
        if response.status_code != 200:
            return None, f"Ø®Ø·Ø£ ÙÙŠ Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø§Ø¨Ø· Ø¥Ù„Ù‰ VirusTotal: {response.status_code}"

        result = response.json()
        url_id = result['data']['id']

        # 2. Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ù„Ø¶Ù…Ø§Ù† Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ÙØ­Øµ (Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ù…ØªØ¹Ø¯Ø¯Ø©)
        max_retries = 3
        report = None
        for _ in range(max_retries):
            time.sleep(15)  # Ø§Ù†ØªØ¸Ø± 15 Ø«Ø§Ù†ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
            report_url = f"https://www.virustotal.com/api/v3/analyses/{url_id}"
            report_response = requests.get(report_url, headers=headers)
            if report_response.status_code == 200:
                report = report_response.json()
                if report['data']['attributes']['status'] == 'completed':
                    break
        else:
            return None, "ÙØ´Ù„ ÙÙŠ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ù…ÙƒØªÙ…Ù„ Ø¨Ø¹Ø¯ Ø¹Ø¯Ø© Ù…Ø­Ø§ÙˆÙ„Ø§Øª"

        # 3. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        stats = report['data']['attributes']['stats']
        result_text = f"""
        <b>Ù†ØªØ§Ø¦Ø¬ VirusTotal:</b>
        <ul>
            <li>Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„ÙØ­ÙˆØµØ§Øª: {sum(stats.values())}</li>
            <li>Ø§Ù„ÙØ­ÙˆØµØ§Øª Ø§Ù„Ø®Ø¨ÙŠØ«Ø©: {stats.get('malicious', 0)}</li>
            <li>Ø§Ù„ÙØ­ÙˆØµØ§Øª Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©: {stats.get('suspicious', 0)}</li>
            <li>Ø§Ù„ÙØ­ÙˆØµØ§Øª Ø§Ù„Ù†Ø¸ÙŠÙØ©: {stats.get('harmless', 0)}</li>
            <li>ØºÙŠØ± Ù…ØµÙ†Ù: {stats.get('undetected', 0)}</li>
        </ul>
        """

        # 4. ØªØ­Ø¯ÙŠØ¯ Ø­Ø§Ù„Ø© Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø¹ ØªÙØ³ÙŠØ± Ø£ÙˆØ¶Ø­
        if stats.get('malicious', 0) > 0:
            return False, f"<b>âš ï¸ Ø§Ù„Ø±Ø§Ø¨Ø· Ø®Ø¨ÙŠØ«:</b><br>{result_text}"
        elif stats.get('suspicious', 0) > 0:
            return False, f"<b>ğŸ” Ø§Ù„Ø±Ø§Ø¨Ø· Ù…Ø´Ø¨ÙˆÙ‡ (Ø£Ø³Ø¨Ø§Ø¨ Ù…Ø­ØªÙ…Ù„Ø©):</b><br>{result_text}"
        else:
            return True, f"<b>âœ… Ø§Ù„Ø±Ø§Ø¨Ø· Ø¢Ù…Ù†:</b><br>{result_text}"

    except Exception as e:
        return None, f"Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ ÙØ­Øµ VirusTotal: {str(e)}"
def analyze_url(url, driver=None, vt_api_key=None):
    # ÙØ­Øµ SSL
    ssl_ok, ssl_msg = check_ssl_cert(url)

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØ§Ù„ØªÙ†Ø¨Ø¤
    features_df = extract_features_from_url(url)
    pred = model.predict(features_df.values)[0]
    label = "Phishing" if pred == -1 else "Legitimate"

    # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
    try:
        resp = requests.get(url, timeout=10)
        if resp.status_code != 200:
            return {
                'error': f"ØªØ¹Ø°Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©ØŒ Ø±Ù…Ø² Ø§Ù„Ø­Ø§Ù„Ø©: {resp.status_code}"
            }
        html_content = resp.text
    except Exception as e:
        return {
            'error': f"Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©: {e}"
        }

    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ
    text_ok, text_msg = analyze_text_content(html_content)

    # ØªØ­Ù„ÙŠÙ„ HTML
    html_ok, html_msg = analyze_html(html_content, url)

    # ÙØ­Øµ Ø¬Ø§ÙØ§Ø³ÙƒØ±ÙŠØ¨Øª
    js_ok = True
    js_msg = "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª Ù…Ø´Ø¨ÙˆÙ‡Ø©"
    if driver:
        try:
            driver.get(url)
            time.sleep(5)
            page_source = driver.page_source
            if "eval(" in page_source or "unescape(" in page_source:
                js_ok = False
                js_msg = "ÙˆØ¬ÙˆØ¯ Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª ØªØ´ÙÙŠØ± Ø£Ùˆ Ø¥Ø®ÙØ§Ø¡ Ù…Ø­ØªÙˆÙ‰ ÙÙŠ Ø¬Ø§ÙØ§Ø³ÙƒØ±ÙŠØ¨Øª"
        except Exception as e:
            js_msg = f"Ø®Ø·Ø£ ÙÙŠ Selenium: {e}"
    else:
        js_msg = "Ù„Ù… ÙŠØªÙ… Ø¥Ø¬Ø±Ø§Ø¡ ÙØ­Øµ Ø¬Ø§ÙØ§Ø³ÙƒØ±ÙŠØ¨Øª"

    # ÙØ­Øµ VirusTotal
    vt_ok = True
    vt_msg = "Ù„Ù… ÙŠØªÙ… ØªÙØ¹ÙŠÙ„ ÙØ­Øµ VirusTotal"
    if vt_api_key:
        vt_result, vt_msg = virustotal_scan(url, vt_api_key)
        if vt_result is None:
            vt_msg = "ØªØ¹Ø°Ø± Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªÙŠØ¬Ø© ÙØ­Øµ VirusTotal"
        elif vt_result is False:
            vt_ok = False

    # Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    if not ssl_ok or pred == -1 or not text_ok or not html_ok or not js_ok or not vt_ok:
        final_result = "Ø±Ø§Ø¨Ø· Ù…Ø´Ø¨ÙˆÙ‡ (Phishing)"
    else:
        final_result = "Ø±Ø§Ø¨Ø· Ø¢Ù…Ù† (Legitimate)"

    return {
        'url': url,
        'ssl_status': ssl_msg,
        'model_prediction': label,
        'content_analysis': text_msg,
        'html_analysis': html_msg,
        'javascript_analysis': js_msg,
        'virustotal_result': vt_msg,
        'final_result': final_result
    }